{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import optimizers, callbacks, losses, layers\n",
    "from tensorflow.keras.layers import Dense, Concatenate, Activation, Add, BatchNormalization, Dropout, Input\n",
    "from tensorflow.keras.models import Model, Sequential, load_model\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "os.environ['PYTHONHASHSEED']=str(SEED)\n",
    "random.seed(SEED)\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    except RuntimeError as e:\n",
    "        # 프로그램 시작시에 메모리 증가가 설정되어야만 합니다\n",
    "        print(e)\n",
    "\n",
    "def mish(x):\n",
    "    return x*tf.math.tanh(tf.math.softplus(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DenseBlock(n, activation='relu', bn=False):\n",
    "    def f(x):\n",
    "        x = Dense(n, kernel_initializer='he_normal')(x)\n",
    "        if bn:\n",
    "            x = BatchNormalization()(x)\n",
    "        x = Activation(activation)(x)\n",
    "        return x\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gamma = 0.9  # Discount factor for past rewards\n",
    "# epsilon = 1.0  # Epsilon greedy parameter\n",
    "# epsilon_min = 0.1  # Minimum epsilon greedy parameter\n",
    "# epsilon_max = 1.0  # Maximum epsilon greedy parameter\n",
    "# epsilon_interval = (\n",
    "#     epsilon_max - epsilon_min\n",
    "# )  # Rate at which to reduce chance of random action being taken\n",
    "# batch_size = 32  # Size of batch taken from replay buffer\n",
    "# max_steps_per_episode = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "env.seed(SEED)\n",
    "\n",
    "num_actions = env.action_space.n\n",
    "num_states = env.observation_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class Memory:\n",
    "    def __init__(self, capacity):\n",
    "        self.actions = deque(maxlen=capacity)\n",
    "        self.states = deque(maxlen=capacity)\n",
    "        self.states_nexts = deque(maxlen=capacity)\n",
    "        self.rewards = deque(maxlen=capacity)\n",
    "        self.dones = deque(maxlen=capacity)\n",
    "    \n",
    "    def store(self, obs): # obs: s, a, r, s'\n",
    "        self.states.append(obs[0])\n",
    "        self.actions.append(obs[1])\n",
    "        self.rewards.append(obs[2])\n",
    "        self.states_nexts.append(obs[3])\n",
    "        self.dones.append(obs[4])\n",
    "    \n",
    "    def extract(self, batch_size):\n",
    "        idx = np.random.choice(range(self.get_len()), batch_size, replace=False)\n",
    "        obs = [\n",
    "            np.array(self.states)[idx],\n",
    "            np.array(self.actions)[idx],\n",
    "            np.array(self.rewards)[idx],\n",
    "            np.array(self.states_nexts)[idx],\n",
    "            np.array(self.dones)[idx],\n",
    "        ]\n",
    "        \n",
    "        return obs\n",
    "\n",
    "    def get_len(self):\n",
    "        return len(self.actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, memory, num_states=num_states):\n",
    "        self.gamma = 0.95\n",
    "        self.memory = memory\n",
    "        \n",
    "        self.eps = 1\n",
    "        self.eps_max = 1\n",
    "        self.eps_min = 0.01\n",
    "        self.eps_interval = self.eps_max - self.eps_min\n",
    "        self.eps_steps = 250\n",
    "        self.steps = 0\n",
    "\n",
    "        self.net = self.build_net(num_states)\n",
    "        self.target_net = self.build_net(num_states)\n",
    "        self.target_net.set_weights(self.net.get_weights())\n",
    "        \n",
    "        self.opt = optimizers.Adam(1e-3)\n",
    "        self.loss_fn = losses.MeanSquaredError()\n",
    "\n",
    "    def build_net(self, num_states):\n",
    "        inputs = Input(shape = (num_states, ))\n",
    "        x = DenseBlock(64)(inputs)\n",
    "        x = DenseBlock(32)(x)\n",
    "        \n",
    "        outputs = Dense(2)(x)\n",
    "        return Model(inputs, outputs)\n",
    "\n",
    "    def train(self, batch_size):\n",
    "        obs = self.memory.extract(batch_size)\n",
    "        \n",
    "        state = tf.convert_to_tensor(obs[0])\n",
    "        action = tf.convert_to_tensor(obs[1])\n",
    "        reward = tf.convert_to_tensor(obs[2])\n",
    "        state_next = tf.convert_to_tensor(obs[3])\n",
    "        done = tf.convert_to_tensor(obs[4])\n",
    "        \n",
    "        futurue_reward = self.target_net.predict(state_next)\n",
    "#         next_action = tf.argmax(target_q, axis=1)\n",
    "        updated_q_values = reward + self.gamma*tf.reduce_max(futurue_reward, axis=1)\n",
    "        updated_q_values = updated_q_values * (1. - done) - done\n",
    "        \n",
    "        masks = tf.one_hot(action, num_actions)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            q_values = self.net(state)\n",
    "            q_actions = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n",
    "            \n",
    "            loss = self.loss_fn(updated_q_values, q_actions)\n",
    "            \n",
    "        grads = tape.gradient(loss, self.net.trainable_weights)\n",
    "        self.opt.apply_gradients(zip(grads, self.net.trainable_weights))\n",
    "        \n",
    "        self.steps += 1  \n",
    "        \n",
    "    def policy(self, state):\n",
    "        rn = np.random.random()\n",
    "        if rn < self.eps:\n",
    "            action =  np.random.randint(0, num_actions)\n",
    "        else:\n",
    "            action = np.argmax(self.net(state))\n",
    "            \n",
    "        self.eps = max(self.eps_min, self.eps_max-self.eps_interval*self.steps/self.eps_steps)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0 reward: 29.0, 1.0, 0\n",
      "episode 1 reward: 8.0, 0.98416, 5\n",
      "episode 2 reward: 17.0, 0.91684, 22\n",
      "episode 3 reward: 21.0, 0.83368, 43\n",
      "episode 4 reward: 11.0, 0.79012, 54\n",
      "episode 5 reward: 19.0, 0.71488, 73\n",
      "episode 6 reward: 19.0, 0.63964, 92\n",
      "episode 7 reward: 14.0, 0.5842, 106\n",
      "episode 8 reward: 9.0, 0.5485599999999999, 115\n",
      "episode 9 reward: 16.0, 0.4852000000000001, 131\n",
      "episode 10 reward: 20.0, 0.406, 151\n",
      "episode 11 reward: 11.0, 0.3624400000000001, 162\n",
      "episode 12 reward: 10.0, 0.32284, 172\n",
      "episode 13 reward: 12.0, 0.27532, 184\n",
      "episode 14 reward: 11.0, 0.23175999999999997, 195\n",
      "episode 15 reward: 10.0, 0.19216, 205\n",
      "episode 16 reward: 11.0, 0.14860000000000007, 216\n",
      "episode 17 reward: 10.0, 0.10899999999999999, 226\n",
      "episode 18 reward: 10.0, 0.06940000000000002, 236\n",
      "episode 19 reward: 9.0, 0.03376000000000001, 245\n",
      "episode 20 reward: 10.0, 0.01, 255\n",
      "episode 21 reward: 10.0, 0.01, 265\n",
      "episode 22 reward: 10.0, 0.01, 275\n",
      "episode 23 reward: 10.0, 0.01, 285\n",
      "episode 24 reward: 10.0, 0.01, 295\n",
      "episode 25 reward: 8.0, 0.01, 303\n",
      "episode 26 reward: 10.0, 0.01, 313\n",
      "episode 27 reward: 11.0, 0.01, 324\n",
      "episode 28 reward: 11.0, 0.01, 335\n",
      "episode 29 reward: 11.0, 0.01, 346\n",
      "episode 30 reward: 9.0, 0.01, 355\n",
      "episode 31 reward: 11.0, 0.01, 366\n",
      "episode 32 reward: 10.0, 0.01, 376\n",
      "episode 33 reward: 12.0, 0.01, 388\n",
      "episode 34 reward: 9.0, 0.01, 397\n",
      "episode 35 reward: 12.0, 0.01, 409\n",
      "episode 36 reward: 14.0, 0.01, 423\n",
      "episode 37 reward: 14.0, 0.01, 437\n",
      "episode 38 reward: 21.0, 0.01, 458\n",
      "episode 39 reward: 18.0, 0.01, 476\n",
      "episode 40 reward: 21.0, 0.01, 497\n",
      "episode 41 reward: 15.0, 0.01, 512\n",
      "episode 42 reward: 15.0, 0.01, 527\n",
      "episode 43 reward: 19.0, 0.01, 546\n",
      "episode 44 reward: 17.0, 0.01, 563\n",
      "episode 45 reward: 19.0, 0.01, 582\n",
      "episode 46 reward: 15.0, 0.01, 597\n",
      "episode 47 reward: 17.0, 0.01, 614\n",
      "episode 48 reward: 25.0, 0.01, 639\n",
      "episode 49 reward: 24.0, 0.01, 663\n",
      "episode 50 reward: 23.0, 0.01, 686\n",
      "episode 51 reward: 21.0, 0.01, 707\n",
      "episode 52 reward: 16.0, 0.01, 723\n",
      "episode 53 reward: 35.0, 0.01, 758\n",
      "episode 54 reward: 15.0, 0.01, 773\n",
      "episode 55 reward: 36.0, 0.01, 809\n",
      "episode 56 reward: 39.0, 0.01, 848\n",
      "episode 57 reward: 22.0, 0.01, 870\n",
      "episode 58 reward: 43.0, 0.01, 913\n",
      "episode 59 reward: 31.0, 0.01, 944\n",
      "episode 60 reward: 39.0, 0.01, 983\n",
      "episode 61 reward: 25.0, 0.01, 1008\n",
      "episode 62 reward: 47.0, 0.01, 1055\n",
      "episode 63 reward: 32.0, 0.01, 1087\n",
      "episode 64 reward: 47.0, 0.01, 1134\n",
      "episode 65 reward: 32.0, 0.01, 1166\n",
      "episode 66 reward: 53.0, 0.01, 1219\n",
      "episode 67 reward: 109.0, 0.01, 1328\n",
      "episode 68 reward: 146.0, 0.01, 1474\n",
      "episode 69 reward: 125.0, 0.01, 1599\n",
      "episode 70 reward: 189.0, 0.01, 1788\n",
      "episode 71 reward: 153.0, 0.01, 1941\n",
      "episode 72 reward: 202.0, 0.01, 2143\n",
      "episode 73 reward: 165.0, 0.01, 2308\n",
      "episode 74 reward: 202.0, 0.01, 2510\n",
      "episode 75 reward: 182.0, 0.01, 2692\n",
      "episode 76 reward: 202.0, 0.01, 2894\n",
      "episode 77 reward: 202.0, 0.01, 3096\n",
      "episode 78 reward: 202.0, 0.01, 3298\n",
      "episode 79 reward: 202.0, 0.01, 3500\n",
      "episode 80 reward: 202.0, 0.01, 3702\n",
      "episode 81 reward: 202.0, 0.01, 3904\n",
      "episode 82 reward: 144.0, 0.01, 4048\n",
      "episode 83 reward: 202.0, 0.01, 4250\n",
      "episode 84 reward: 202.0, 0.01, 4452\n",
      "episode 85 reward: 202.0, 0.01, 4654\n",
      "episode 86 reward: 202.0, 0.01, 4856\n",
      "episode 87 reward: 202.0, 0.01, 5058\n",
      "episode 88 reward: 202.0, 0.01, 5260\n",
      "episode 89 reward: 202.0, 0.01, 5462\n",
      "episode 90 reward: 202.0, 0.01, 5664\n",
      "episode 91 reward: 202.0, 0.01, 5866\n",
      "episode 92 reward: 202.0, 0.01, 6068\n",
      "episode 93 reward: 202.0, 0.01, 6270\n",
      "episode 94 reward: 202.0, 0.01, 6472\n",
      "episode 95 reward: 202.0, 0.01, 6674\n",
      "episode 96 reward: 202.0, 0.01, 6876\n",
      "episode 97 reward: 202.0, 0.01, 7078\n",
      "episode 98 reward: 202.0, 0.01, 7280\n",
      "episode 99 reward: 202.0, 0.01, 7482\n",
      "episode 100 reward: 202.0, 0.01, 7684\n",
      "episode 101 reward: 202.0, 0.01, 7886\n",
      "episode 102 reward: 202.0, 0.01, 8088\n",
      "episode 103 reward: 202.0, 0.01, 8290\n",
      "episode 104 reward: 202.0, 0.01, 8492\n",
      "episode 105 reward: 202.0, 0.01, 8694\n",
      "episode 106 reward: 202.0, 0.01, 8896\n",
      "episode 107 reward: 202.0, 0.01, 9098\n",
      "episode 108 reward: 202.0, 0.01, 9300\n",
      "episode 109 reward: 202.0, 0.01, 9502\n",
      "episode 110 reward: 159.0, 0.01, 9661\n",
      "episode 111 reward: 202.0, 0.01, 9863\n",
      "episode 112 reward: 202.0, 0.01, 10065\n",
      "episode 113 reward: 180.0, 0.01, 10245\n",
      "episode 114 reward: 143.0, 0.01, 10388\n",
      "episode 115 reward: 121.0, 0.01, 10509\n",
      "episode 116 reward: 141.0, 0.01, 10650\n",
      "episode 117 reward: 202.0, 0.01, 10852\n",
      "episode 118 reward: 202.0, 0.01, 11054\n",
      "episode 119 reward: 194.0, 0.01, 11248\n",
      "episode 120 reward: 170.0, 0.01, 11418\n",
      "episode 121 reward: 202.0, 0.01, 11620\n",
      "episode 122 reward: 202.0, 0.01, 11822\n",
      "episode 123 reward: 201.0, 0.01, 12023\n",
      "episode 124 reward: 202.0, 0.01, 12225\n",
      "episode 125 reward: 202.0, 0.01, 12427\n",
      "episode 126 reward: 202.0, 0.01, 12629\n",
      "episode 127 reward: 202.0, 0.01, 12831\n",
      "episode 128 reward: 202.0, 0.01, 13033\n",
      "episode 129 reward: 202.0, 0.01, 13235\n",
      "episode 130 reward: 202.0, 0.01, 13437\n",
      "episode 131 reward: 202.0, 0.01, 13639\n",
      "episode 132 reward: 202.0, 0.01, 13841\n",
      "episode 133 reward: 202.0, 0.01, 14043\n",
      "episode 134 reward: 202.0, 0.01, 14245\n",
      "episode 135 reward: 202.0, 0.01, 14447\n",
      "episode 136 reward: 202.0, 0.01, 14649\n",
      "episode 137 reward: 202.0, 0.01, 14851\n",
      "episode 138 reward: 202.0, 0.01, 15053\n",
      "episode 139 reward: 202.0, 0.01, 15255\n",
      "episode 140 reward: 202.0, 0.01, 15457\n",
      "episode 141 reward: 202.0, 0.01, 15659\n",
      "episode 142 reward: 202.0, 0.01, 15861\n",
      "episode 143 reward: 202.0, 0.01, 16063\n",
      "episode 144 reward: 202.0, 0.01, 16265\n",
      "episode 145 reward: 202.0, 0.01, 16467\n",
      "episode 146 reward: 202.0, 0.01, 16669\n",
      "episode 147 reward: 202.0, 0.01, 16871\n",
      "episode 148 reward: 202.0, 0.01, 17073\n",
      "episode 149 reward: 202.0, 0.01, 17275\n"
     ]
    }
   ],
   "source": [
    "n_episode = 150\n",
    "\n",
    "batch_size = 32\n",
    "ep_reward_list = []\n",
    "\n",
    "memory = Memory(2000)\n",
    "agent = Agent(memory)\n",
    "\n",
    "for e in range(n_episode):\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    \n",
    "    steps = 0\n",
    "    while 1:\n",
    "        state_ = tf.expand_dims(tf.convert_to_tensor(state), 0)\n",
    "        action = agent.policy(state_)\n",
    "        \n",
    "        state_next, reward, done, _ = env.step(action)\n",
    "        reward = tf.cast(reward, dtype=tf.float32)\n",
    "        done = tf.cast(done, dtype=tf.float32)\n",
    "        agent.memory.store((state, action, reward, state_next, done))\n",
    "        \n",
    "        episode_reward += reward\n",
    "        \n",
    "        state = state_next\n",
    "        \n",
    "        if agent.memory.get_len() > batch_size:\n",
    "            agent.train(batch_size)\n",
    "\n",
    "        if done or steps>200:\n",
    "            agent.target_net.set_weights(agent.net.get_weights())\n",
    "            break\n",
    "            \n",
    "        steps += 1\n",
    "    ep_reward_list.append(episode_reward)\n",
    "    print(f'episode {e} reward: {episode_reward}, {agent.eps}, {agent.steps}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
