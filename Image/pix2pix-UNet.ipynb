{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "from tensorflow.keras import optimizers, callbacks, losses\n",
    "from tensorflow.keras.layers import Dense, Concatenate, Activation, Add, BatchNormalization, Dropout, Conv2D, MaxPooling2D, Conv2DTranspose,\\\n",
    "concatenate, Input, UpSampling2D\n",
    "from tensorflow.keras.models import Model, Sequential, load_model\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import random\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "os.environ['PYTHONHASHSEED']=str(SEED)\n",
    "random.seed(SEED)\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    except RuntimeError as e:\n",
    "        # 프로그램 시작시에 메모리 증가가 설정되어야만 합니다\n",
    "        print(e)\n",
    "\n",
    "def mish(x):\n",
    "    return x*tf.math.tanh(tf.math.softplus(x))\n",
    "\n",
    "def decay(epochs):\n",
    "    init = 1e-3\n",
    "    drop = 10\n",
    "    ratio = 0.9\n",
    "    return max(5e-5, (init * (ratio ** (epochs//drop))))\n",
    "\n",
    "es = callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "lrs = callbacks.LearningRateScheduler(decay, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(keras.Model):\n",
    "    def __init__(self, x_shape, y_shape):\n",
    "        super(GAN, self).__init__()\n",
    "        self.x_shape  = x_shape\n",
    "        self.y_shape = y_shape\n",
    "        \n",
    "        self.df = 64\n",
    "        self.gf = 64\n",
    "        \n",
    "        self.generator = self.build_generator()\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        \n",
    "\n",
    "    \n",
    "    def compile(self, g_optim, d_optim, d_loss_fn, recon_loss_fn):\n",
    "        super(GAN, self).compile()\n",
    "        self.g_optim = g_optim\n",
    "        self.d_optim = d_optim\n",
    "        self.d_loss_fn = d_loss_fn\n",
    "        self.recon_loss_fn = recon_loss_fn\n",
    "        \n",
    "    def build_generator(self):\n",
    "        start_neurons = 64\n",
    "        \n",
    "        inputs = Input(shape=self.x_shape)\n",
    "        \n",
    "        conv1 = Conv2D(start_neurons * 1, (3, 3), activation=mish, padding=\"same\")(inputs)\n",
    "        pool1 = BatchNormalization()(conv1)\n",
    "        pool1 = MaxPooling2D((2, 2))(pool1)\n",
    "\n",
    "        conv2 = Conv2D(start_neurons * 2, (3, 3), activation=mish, padding=\"same\")(pool1)\n",
    "        pool2 = BatchNormalization()(conv2)\n",
    "        pool2 = MaxPooling2D((2, 2))(pool2)\n",
    "\n",
    "        convm = Conv2D(start_neurons * 4, (3, 3), activation=mish, padding=\"same\")(pool2)\n",
    "\n",
    "        deconv2 = Conv2DTranspose(start_neurons * 2, (3, 3), strides=(2, 2), padding=\"same\")(convm)\n",
    "        uconv2 = concatenate([deconv2, conv2])\n",
    "        uconv2 = Conv2D(start_neurons * 2, (3, 3), activation=mish, padding=\"same\")(uconv2)\n",
    "        uconv2 = BatchNormalization()(uconv2)\n",
    "\n",
    "        deconv1 = Conv2DTranspose(start_neurons * 1, (3, 3), strides=(2, 2), padding=\"same\")(uconv2)\n",
    "        uconv1 = concatenate([deconv1, conv1])\n",
    "        uconv1 = Conv2D(start_neurons * 1, (3, 3), activation=mish, padding=\"same\")(uconv1)\n",
    "        uconv1 = BatchNormalization()(uconv1)\n",
    "        outputs = Conv2D(1, (1,1), padding=\"same\", activation='relu', dtype='float32')(uconv1)\n",
    "        \n",
    "        return Model(inputs, outputs)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        def d_layer(layer_input, filters, f_size=4, bn=True):\n",
    "            \"\"\"Discriminator layer\"\"\"\n",
    "            d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n",
    "            d = Activation(mish)(d)\n",
    "            if bn:\n",
    "                d = BatchNormalization(momentum=0.8)(d)\n",
    "            return d\n",
    "\n",
    "        img_A = Input(shape=self.x_shape)\n",
    "        img_B = Input(shape=self.y_shape)\n",
    "\n",
    "        combined_imgs = concatenate([img_A, img_B])\n",
    "\n",
    "        d1 = d_layer(combined_imgs, self.df, bn=False)\n",
    "        d2 = d_layer(d1, self.df*2)\n",
    "        d3 = d_layer(d2, self.df*4)\n",
    "        d4 = d_layer(d3, self.df*8)\n",
    "\n",
    "        validity = Conv2D(1, kernel_size=4, strides=1, padding='same', dtype='float32')(d4)\n",
    "\n",
    "        return Model([img_A, img_B], validity)\n",
    "    \n",
    "    def train_step(self, data):\n",
    "        x, y = data\n",
    "        batch_size = tf.shape(x)[0]\n",
    "        \n",
    "        preds_y = self.generator(x)\n",
    "        all_y = tf.concat([y, preds_y], 0)\n",
    "        all_x = tf.concat([x, x], 0)\n",
    "        \n",
    "        fake_labels = tf.ones((batch_size, 1))*0\n",
    "        real_labels = tf.ones((batch_size, 1))\n",
    "        labels = tf.concat([fake_labels, real_labels], 0)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            preds_ = self.generator(x)\n",
    "            validity, _ = self.discriminator([all_x, all_y])\n",
    "            d_loss = self.d_loss_fn(labels, validity)\n",
    "            \n",
    "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
    "        self.d_optim.apply_gradients(zip(grads, self.discriminator.trainable_weights))\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            preds = self.generator(x)\n",
    "            validity, preds = self.discriminator([x, preds])\n",
    "            g_loss = self.recon_loss_fn([real_labels, y], [validity, preds])\n",
    "            \n",
    "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
    "        self.g_optim.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
    "\n",
    "        return {'d_loss': d_loss, 'g_loss': g_loss}\n",
    "    \n",
    "    def sampler(self, batch_size):\n",
    "        return tf.random.normal(shape=(batch_size, self.z_dim))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recon_loss(y_trues, y_preds):\n",
    "    t1, t2 = y_trues\n",
    "    p1, p2 = y_preds\n",
    "    \n",
    "    validity_loss = losses.BinaryCrossentropy()(t1, p1)\n",
    "    recon_loss = losses.MeanAbsoluteError()(t2, p2)\n",
    "    \n",
    "    return validity_loss+recon_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan = GAN((120, 120, 4), (120, 120, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan.compile(\n",
    "    optimizers.Adam(2e-4),\n",
    "    optimizers.Adam(2e-4),\n",
    "    losses.BinaryCrossentropy(),\n",
    "    recon_loss\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 120, 120, 4) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 120, 120, 64) 2368        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 120, 120, 64) 256         conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 60, 60, 64)   0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 60, 60, 128)  73856       max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 60, 60, 128)  512         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 30, 30, 128)  0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 30, 30, 256)  295168      max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose (Conv2DTranspo (None, 60, 60, 128)  295040      conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 60, 60, 256)  0           conv2d_transpose[0][0]           \n",
      "                                                                 conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 60, 60, 128)  295040      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 60, 60, 128)  512         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTrans (None, 120, 120, 64) 73792       batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 120, 120, 128 0           conv2d_transpose_1[0][0]         \n",
      "                                                                 conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 120, 120, 64) 73792       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 120, 120, 64) 256         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 120, 120, 1)  65          batch_normalization_3[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 1,110,657\n",
      "Trainable params: 1,109,889\n",
      "Non-trainable params: 768\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gan.generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 120, 120, 4) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 120, 120, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 120, 120, 5)  0           input_2[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 60, 60, 64)   5184        concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 60, 60, 64)   0           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 30, 30, 128)  131200      activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 30, 30, 128)  0           conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 30, 30, 128)  512         activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 15, 15, 256)  524544      batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 15, 15, 256)  0           conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 15, 15, 256)  1024        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 8, 8, 512)    2097664     batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 8, 8, 512)    0           conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 8, 8, 512)    2048        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 8, 8, 1)      8193        batch_normalization_6[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 2,770,369\n",
      "Trainable params: 2,768,577\n",
      "Non-trainable params: 1,792\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gan.discriminator.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
